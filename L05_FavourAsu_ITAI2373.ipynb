{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10025,"databundleVersionId":32092,"sourceType":"competition"},{"sourceId":4304,"sourceType":"datasetVersion","datasetId":2568},{"sourceId":19053,"sourceType":"datasetVersion","datasetId":14154}],"dockerImageVersionId":18199,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## General information\n\nIn this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n\nThis dataset is interesting for NLP researching. Sentences from original dataset were split in separate phrases and each of them has a sentiment label. Also a lot of phrases are really short which makes classifying them quite challenging. Let's try!","metadata":{"_uuid":"839fc1317e1b7253241839bbfa2d40303c53a3f1"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-03-20T01:26:06.918127Z","iopub.execute_input":"2024-03-20T01:26:06.918507Z","iopub.status.idle":"2024-03-20T01:26:06.927683Z","shell.execute_reply.started":"2024-03-20T01:26:06.918446Z","shell.execute_reply":"2024-03-20T01:26:06.926730Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/movie-review-sentiment-analysis-kernels-only/train.tsv.zip', sep=\"\\t\")\ntest = pd.read_csv('/kaggle/input/movie-review-sentiment-analysis-kernels-only/test.tsv.zip', sep=\"\\t\")\nsub = pd.read_csv('/kaggle/input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv', sep=\",\")","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"execution":{"iopub.status.busy":"2024-03-20T01:26:06.928828Z","iopub.execute_input":"2024-03-20T01:26:06.929099Z","iopub.status.idle":"2024-03-20T01:26:07.314062Z","shell.execute_reply.started":"2024-03-20T01:26:06.929034Z","shell.execute_reply":"2024-03-20T01:26:07.313300Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"_uuid":"f9b8d8423bb09068cb168b67f4756ee8b250fc8c","execution":{"iopub.status.busy":"2024-03-20T01:26:07.315374Z","iopub.execute_input":"2024-03-20T01:26:07.315592Z","iopub.status.idle":"2024-03-20T01:26:07.347087Z","shell.execute_reply.started":"2024-03-20T01:26:07.315554Z","shell.execute_reply":"2024-03-20T01:26:07.346222Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   PhraseId    ...      Sentiment\n0         1    ...              1\n1         2    ...              2\n2         3    ...              2\n3         4    ...              2\n4         5    ...              2\n5         6    ...              2\n6         7    ...              2\n7         8    ...              2\n8         9    ...              2\n9        10    ...              2\n\n[10 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>A series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>series</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>1</td>\n      <td>of escapades demonstrating the adage that what is good for the goose</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>of</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1</td>\n      <td>escapades demonstrating the adage that what is good for the goose</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1</td>\n      <td>escapades</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1</td>\n      <td>demonstrating the adage that what is good for the goose</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.loc[train.SentenceId == 2]","metadata":{"_uuid":"e94f8371d8be87186f23ecc75178480d3d96bd78","execution":{"iopub.status.busy":"2024-03-20T01:26:07.348596Z","iopub.execute_input":"2024-03-20T01:26:07.348915Z","iopub.status.idle":"2024-03-20T01:26:07.369510Z","shell.execute_reply.started":"2024-03-20T01:26:07.348859Z","shell.execute_reply":"2024-03-20T01:26:07.368583Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"    PhraseId    ...      Sentiment\n63        64    ...              4\n64        65    ...              3\n65        66    ...              2\n66        67    ...              4\n67        68    ...              3\n68        69    ...              2\n69        70    ...              3\n70        71    ...              3\n71        72    ...              3\n72        73    ...              2\n73        74    ...              2\n74        75    ...              4\n75        76    ...              2\n76        77    ...              3\n77        78    ...              4\n78        79    ...              2\n79        80    ...              2\n80        81    ...              2\n\n[18 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>63</th>\n      <td>64</td>\n      <td>2</td>\n      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>65</td>\n      <td>2</td>\n      <td>This quiet , introspective and entertaining independent</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>66</td>\n      <td>2</td>\n      <td>This</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>67</td>\n      <td>2</td>\n      <td>quiet , introspective and entertaining independent</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>68</td>\n      <td>2</td>\n      <td>quiet , introspective and entertaining</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>69</td>\n      <td>2</td>\n      <td>quiet</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>70</td>\n      <td>2</td>\n      <td>, introspective and entertaining</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>71</td>\n      <td>2</td>\n      <td>introspective and entertaining</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>72</td>\n      <td>2</td>\n      <td>introspective and</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>73</td>\n      <td>2</td>\n      <td>introspective</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>74</td>\n      <td>2</td>\n      <td>and</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>75</td>\n      <td>2</td>\n      <td>entertaining</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>76</td>\n      <td>2</td>\n      <td>independent</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>77</td>\n      <td>2</td>\n      <td>is worth seeking .</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>78</td>\n      <td>2</td>\n      <td>is worth seeking</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>79</td>\n      <td>2</td>\n      <td>is worth</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>80</td>\n      <td>2</td>\n      <td>worth</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>81</td>\n      <td>2</td>\n      <td>seeking</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))","metadata":{"_uuid":"4f75e24b86e3aeb7477fa1cd69789992233420b1","execution":{"iopub.status.busy":"2024-03-20T01:26:07.370630Z","iopub.execute_input":"2024-03-20T01:26:07.371128Z","iopub.status.idle":"2024-03-20T01:26:07.408972Z","shell.execute_reply.started":"2024-03-20T01:26:07.370848Z","shell.execute_reply":"2024-03-20T01:26:07.408335Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Average count of phrases per sentence in train is 18.\nAverage count of phrases per sentence in test is 20.\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))","metadata":{"_uuid":"ca3148a6bbdd0f71e4909feb5dca874fa6d64a2e","execution":{"iopub.status.busy":"2024-03-20T01:26:07.410088Z","iopub.execute_input":"2024-03-20T01:26:07.410338Z","iopub.status.idle":"2024-03-20T01:26:07.416856Z","shell.execute_reply.started":"2024-03-20T01:26:07.410292Z","shell.execute_reply":"2024-03-20T01:26:07.416255Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Number of phrases in train: 156060. Number of sentences in train: 8529.\nNumber of phrases in test: 66292. Number of sentences in test: 3310.\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))","metadata":{"_uuid":"bef53d8f659b78e3fc6b1ed07025591d55b7c128","execution":{"iopub.status.busy":"2024-03-20T01:26:07.417946Z","iopub.execute_input":"2024-03-20T01:26:07.418178Z","iopub.status.idle":"2024-03-20T01:26:07.723824Z","shell.execute_reply.started":"2024-03-20T01:26:07.418132Z","shell.execute_reply":"2024-03-20T01:26:07.723058Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Average word length of phrases in train is 7.\nAverage word length of phrases in test is 7.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see than sentences were split in 18-20 phrases at average and a lot of phrases contain each other. Sometimes one word or even one punctuation mark influences the sentiment","metadata":{"_uuid":"9ee27697b41dfdca9cdb482bfc85cfd3d63ae6e2"}},{"cell_type":"markdown","source":"Let's see for example most common trigrams for positive phrases","metadata":{"_uuid":"9d1efbed65f250d37472544f4fe37cb6fd13e183","trusted":true}},{"cell_type":"code","source":"text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext_trigrams = [i for i in ngrams(text.split(), 3)]","metadata":{"_uuid":"4a128ab1d36eef6ec47161705a2b1094b830fe10","execution":{"iopub.status.busy":"2024-03-20T01:26:07.724965Z","iopub.execute_input":"2024-03-20T01:26:07.725177Z","iopub.status.idle":"2024-03-20T01:26:07.807085Z","shell.execute_reply.started":"2024-03-20T01:26:07.725140Z","shell.execute_reply":"2024-03-20T01:26:07.806440Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"Counter(text_trigrams).most_common(30)","metadata":{"_uuid":"b5c8533c5861794d44b53dbb2e5ef764e5e88119","execution":{"iopub.status.busy":"2024-03-20T01:26:07.808379Z","iopub.execute_input":"2024-03-20T01:26:07.808669Z","iopub.status.idle":"2024-03-20T01:26:07.863195Z","shell.execute_reply.started":"2024-03-20T01:26:07.808616Z","shell.execute_reply":"2024-03-20T01:26:07.862463Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[(('one', 'of', 'the'), 199),\n (('of', 'the', 'year'), 103),\n (('.', 'is', 'a'), 87),\n (('of', 'the', 'best'), 80),\n (('of', 'the', 'most'), 70),\n (('is', 'one', 'of'), 50),\n (('One', 'of', 'the'), 43),\n ((',', 'and', 'the'), 40),\n (('the', 'year', \"'s\"), 38),\n (('It', \"'s\", 'a'), 38),\n (('it', \"'s\", 'a'), 37),\n (('.', \"'s\", 'a'), 37),\n (('a', 'movie', 'that'), 35),\n (('the', 'edge', 'of'), 34),\n (('the', 'kind', 'of'), 33),\n (('of', 'your', 'seat'), 33),\n (('the', 'film', 'is'), 31),\n ((',', 'this', 'is'), 31),\n (('the', 'film', \"'s\"), 31),\n ((',', 'the', 'film'), 30),\n (('film', 'that', 'is'), 30),\n (('as', 'one', 'of'), 30),\n (('edge', 'of', 'your'), 29),\n ((',', 'it', \"'s\"), 27),\n (('a', 'film', 'that'), 27),\n (('as', 'well', 'as'), 27),\n ((',', 'funny', ','), 25),\n ((',', 'but', 'it'), 23),\n (('films', 'of', 'the'), 23),\n (('some', 'of', 'the'), 23)]"},"metadata":{}}]},{"cell_type":"code","source":"text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\ntext = [i for i in text.split() if i not in stopwords.words('english')]\ntext_trigrams = [i for i in ngrams(text, 3)]\nCounter(text_trigrams).most_common(30)","metadata":{"_uuid":"402c0b58fe43a680bea33e1813012bec5c16cb55","execution":{"iopub.status.busy":"2024-03-20T01:26:07.864427Z","iopub.execute_input":"2024-03-20T01:26:07.864648Z","iopub.status.idle":"2024-03-20T01:26:22.262155Z","shell.execute_reply.started":"2024-03-20T01:26:07.864611Z","shell.execute_reply":"2024-03-20T01:26:22.261339Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[((',', 'funny', ','), 33),\n (('one', 'year', \"'s\"), 28),\n (('year', \"'s\", 'best'), 26),\n (('movies', 'ever', 'made'), 19),\n ((',', 'solid', 'cast'), 19),\n (('solid', 'cast', ','), 18),\n ((\"'ve\", 'ever', 'seen'), 16),\n (('.', 'It', \"'s\"), 16),\n ((',', 'making', 'one'), 15),\n (('best', 'films', 'year'), 15),\n ((',', 'touching', ','), 15),\n (('exquisite', 'acting', ','), 15),\n (('acting', ',', 'inventive'), 14),\n ((',', 'inventive', 'screenplay'), 14),\n (('jaw-dropping', 'action', 'sequences'), 14),\n (('good', 'acting', ','), 14),\n ((\"'s\", 'best', 'films'), 14),\n (('I', \"'ve\", 'seen'), 14),\n (('funny', ',', 'even'), 14),\n (('best', 'war', 'movies'), 13),\n (('purely', 'enjoyable', 'satisfying'), 13),\n (('funny', ',', 'touching'), 13),\n ((',', 'smart', ','), 13),\n (('inventive', 'screenplay', ','), 13),\n (('funniest', 'jokes', 'movie'), 13),\n (('action', 'sequences', ','), 13),\n (('sequences', ',', 'striking'), 13),\n ((',', 'striking', 'villains'), 13),\n (('exquisite', 'motion', 'picture'), 13),\n (('war', 'movies', 'ever'), 12)]"},"metadata":{}}]},{"cell_type":"markdown","source":"The results show the main problem with this dataset: there are to many common words due to sentenced splitted in phrases. As a result stopwords shouldn't be removed from text.","metadata":{"_uuid":"f59db6ed32fafb024728fd95b96157f278682a74"}},{"cell_type":"markdown","source":"### Thoughts on feature processing and engineering","metadata":{"_uuid":"10dc8fc8d535ef492a3dab1b85b4052feec756ee"}},{"cell_type":"markdown","source":"So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n- puntuation could be important, so it should be used;\n- ngrams are necessary to get the most info from data;\n- using features like word count or sentence length won't be useful;","metadata":{"_uuid":"d5d018a694d10cbd9e8e89c26d5227fdb9cf8c0b"}},{"cell_type":"code","source":"tokenizer = TweetTokenizer()","metadata":{"_uuid":"bd603ad818970c3c8c6db5e430a6cb8ae8eafbd5","execution":{"iopub.status.busy":"2024-03-20T01:26:22.263323Z","iopub.execute_input":"2024-03-20T01:26:22.263533Z","iopub.status.idle":"2024-03-20T01:26:22.267226Z","shell.execute_reply.started":"2024-03-20T01:26:22.263496Z","shell.execute_reply":"2024-03-20T01:26:22.266456Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\nvectorizer.fit(full_text)\ntrain_vectorized = vectorizer.transform(train['Phrase'])\ntest_vectorized = vectorizer.transform(test['Phrase'])","metadata":{"_uuid":"7ebc937fd5ec811bc5c529ff416180fe338d073d","execution":{"iopub.status.busy":"2024-03-20T01:26:22.268355Z","iopub.execute_input":"2024-03-20T01:26:22.268563Z","iopub.status.idle":"2024-03-20T01:26:52.842928Z","shell.execute_reply.started":"2024-03-20T01:26:22.268526Z","shell.execute_reply":"2024-03-20T01:26:52.842237Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"y = train['Sentiment']","metadata":{"_uuid":"3d0bb4539b1e0b5f8439878a967ce7b5ece23f60","execution":{"iopub.status.busy":"2024-03-20T01:26:52.844302Z","iopub.execute_input":"2024-03-20T01:26:52.844581Z","iopub.status.idle":"2024-03-20T01:26:52.848396Z","shell.execute_reply.started":"2024-03-20T01:26:52.844526Z","shell.execute_reply":"2024-03-20T01:26:52.847524Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"logreg = LogisticRegression()\novr = OneVsRestClassifier(logreg)","metadata":{"_uuid":"75a7acfd815fb391cad92eed8d2f13e5c9801de8","execution":{"iopub.status.busy":"2024-03-20T01:26:52.849816Z","iopub.execute_input":"2024-03-20T01:26:52.850137Z","iopub.status.idle":"2024-03-20T01:26:52.860769Z","shell.execute_reply.started":"2024-03-20T01:26:52.850078Z","shell.execute_reply":"2024-03-20T01:26:52.859980Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"%%time\novr.fit(train_vectorized, y)","metadata":{"_uuid":"5a0bf05b880e0d05da378b753394e5a631753e82","execution":{"iopub.status.busy":"2024-03-20T01:26:52.862107Z","iopub.execute_input":"2024-03-20T01:26:52.862341Z","iopub.status.idle":"2024-03-20T01:27:01.408163Z","shell.execute_reply.started":"2024-03-20T01:26:52.862302Z","shell.execute_reply":"2024-03-20T01:27:01.407366Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 8.52 s, sys: 12.8 ms, total: 8.53 s\nWall time: 8.53 s\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n          tol=0.0001, verbose=0, warm_start=False),\n          n_jobs=None)"},"metadata":{}}]},{"cell_type":"code","source":"scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))","metadata":{"_uuid":"5946485410c25ae5033bf39e29f49f606ea87bfe","execution":{"iopub.status.busy":"2024-03-20T01:27:01.409364Z","iopub.execute_input":"2024-03-20T01:27:01.409567Z","iopub.status.idle":"2024-03-20T01:27:16.035400Z","shell.execute_reply.started":"2024-03-20T01:27:01.409531Z","shell.execute_reply":"2024-03-20T01:27:16.033077Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Cross-validation mean accuracy 56.55%, std 0.07.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nsvc = LinearSVC(dual=False)\nscores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\nprint('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))","metadata":{"_uuid":"755c2b04f7bf8e86e9dc6242f392a575c61a052c","execution":{"iopub.status.busy":"2024-03-20T01:27:16.040266Z","iopub.execute_input":"2024-03-20T01:27:16.040636Z","iopub.status.idle":"2024-03-20T01:27:30.613312Z","shell.execute_reply.started":"2024-03-20T01:27:16.040570Z","shell.execute_reply":"2024-03-20T01:27:30.612428Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Cross-validation mean accuracy 56.51%, std 0.68.\nCPU times: user 72.7 ms, sys: 19.1 ms, total: 91.8 ms\nWall time: 14.6 s\n","output_type":"stream"}]},{"cell_type":"code","source":"ovr.fit(train_vectorized, y);\nsvc.fit(train_vectorized, y);","metadata":{"_uuid":"55a5845de3412244cfa9f0a11392e50c76d78184","execution":{"iopub.status.busy":"2024-03-20T01:27:30.615064Z","iopub.execute_input":"2024-03-20T01:27:30.615386Z","iopub.status.idle":"2024-03-20T01:27:56.445588Z","shell.execute_reply.started":"2024-03-20T01:27:30.615325Z","shell.execute_reply":"2024-03-20T01:27:56.444967Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Deep learning\nAnd now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition.","metadata":{"_uuid":"398461363c7a395e2a982e07e8ac6fccaee139c1"}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","metadata":{"_uuid":"eb29ec027df57f6597dbef976645dc8d151e1618","execution":{"iopub.status.busy":"2024-03-20T01:27:56.446756Z","iopub.execute_input":"2024-03-20T01:27:56.446972Z","iopub.status.idle":"2024-03-20T01:27:56.537906Z","shell.execute_reply.started":"2024-03-20T01:27:56.446935Z","shell.execute_reply":"2024-03-20T01:27:56.536790Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"tk = Tokenizer(lower = True, filters='')\ntk.fit_on_texts(full_text)","metadata":{"_uuid":"a2881c29f82578b4a373b52d2c7b96a2e73bfd80","execution":{"iopub.status.busy":"2024-03-20T01:27:56.539393Z","iopub.execute_input":"2024-03-20T01:27:56.539644Z","iopub.status.idle":"2024-03-20T01:27:59.134337Z","shell.execute_reply.started":"2024-03-20T01:27:56.539594Z","shell.execute_reply":"2024-03-20T01:27:59.133555Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_tokenized = tk.texts_to_sequences(train['Phrase'])\ntest_tokenized = tk.texts_to_sequences(test['Phrase'])","metadata":{"_uuid":"1724669c7ca010d1bbf75e200211afdead768d1f","execution":{"iopub.status.busy":"2024-03-20T01:27:59.135445Z","iopub.execute_input":"2024-03-20T01:27:59.135670Z","iopub.status.idle":"2024-03-20T01:28:01.707645Z","shell.execute_reply.started":"2024-03-20T01:27:59.135632Z","shell.execute_reply":"2024-03-20T01:28:01.707026Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"max_len = 50\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)","metadata":{"_uuid":"bcb80cf8a59ca779a0be1ab235a1e9da2f4b175b","execution":{"iopub.status.busy":"2024-03-20T01:28:01.709057Z","iopub.execute_input":"2024-03-20T01:28:01.709326Z","iopub.status.idle":"2024-03-20T01:28:03.370512Z","shell.execute_reply.started":"2024-03-20T01:28:01.709276Z","shell.execute_reply":"2024-03-20T01:28:03.369903Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"","metadata":{"_uuid":"9dfd0b8fa2c79bfa206d2fe8e35fbec444418f5c","execution":{"iopub.status.busy":"2024-03-20T01:28:03.371928Z","iopub.execute_input":"2024-03-20T01:28:03.372154Z","iopub.status.idle":"2024-03-20T01:28:03.375378Z","shell.execute_reply.started":"2024-03-20T01:28:03.372114Z","shell.execute_reply":"2024-03-20T01:28:03.374694Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"embed_size = 300\nmax_features = 30000","metadata":{"_uuid":"d74c2aa2f13e8544f045e5644d5bac70e248a8bd","execution":{"iopub.status.busy":"2024-03-20T01:28:03.376674Z","iopub.execute_input":"2024-03-20T01:28:03.376928Z","iopub.status.idle":"2024-03-20T01:28:03.388589Z","shell.execute_reply.started":"2024-03-20T01:28:03.376881Z","shell.execute_reply":"2024-03-20T01:28:03.387817Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","metadata":{"_uuid":"cdb522c23b75331481145b789cf127b39d47eaa1","execution":{"iopub.status.busy":"2024-03-20T01:28:03.389739Z","iopub.execute_input":"2024-03-20T01:28:03.390008Z","iopub.status.idle":"2024-03-20T01:31:53.694684Z","shell.execute_reply.started":"2024-03-20T01:28:03.389958Z","shell.execute_reply":"2024-03-20T01:31:53.693919Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1))","metadata":{"_uuid":"365c0d607d55a78c5890268b9c168eb12a211855","execution":{"iopub.status.busy":"2024-03-20T01:31:53.695750Z","iopub.execute_input":"2024-03-20T01:31:53.696004Z","iopub.status.idle":"2024-03-20T01:31:53.731482Z","shell.execute_reply.started":"2024-03-20T01:31:53.695963Z","shell.execute_reply":"2024-03-20T01:31:53.730771Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\nIf you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\nIn case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n  warnings.warn(msg, FutureWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n    \n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","metadata":{"_uuid":"594273c2d887315d083a35a5ffd7c2dd40c2ebb6","execution":{"iopub.status.busy":"2024-03-20T01:31:53.732895Z","iopub.execute_input":"2024-03-20T01:31:53.733143Z","iopub.status.idle":"2024-03-20T01:31:53.745997Z","shell.execute_reply.started":"2024-03-20T01:31:53.733094Z","shell.execute_reply":"2024-03-20T01:31:53.745290Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"An attempt at ensemble:","metadata":{"_uuid":"95f52b1f6de4e939c8d21e3525503912282fbd47"}},{"cell_type":"code","source":"model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)","metadata":{"_uuid":"5eb586c98fb75c25cac099cd03d8233185fdc317","execution":{"iopub.status.busy":"2024-03-20T01:31:53.747491Z","iopub.execute_input":"2024-03-20T01:31:53.747786Z","iopub.status.idle":"2024-03-20T01:36:31.199986Z","shell.execute_reply.started":"2024-03-20T01:31:53.747732Z","shell.execute_reply":"2024-03-20T01:36:31.199304Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Train on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 30s 213us/step - loss: 0.3635 - acc: 0.8301 - val_loss: 0.3168 - val_acc: 0.8521\n\nEpoch 00001: val_loss improved from inf to 0.31679, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 23s 162us/step - loss: 0.3113 - acc: 0.8583 - val_loss: 0.3103 - val_acc: 0.8556\n\nEpoch 00002: val_loss improved from 0.31679 to 0.31026, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 23s 161us/step - loss: 0.2996 - acc: 0.8634 - val_loss: 0.3079 - val_acc: 0.8573\n\nEpoch 00003: val_loss improved from 0.31026 to 0.30788, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 23s 160us/step - loss: 0.2914 - acc: 0.8664 - val_loss: 0.3054 - val_acc: 0.8596\n\nEpoch 00004: val_loss improved from 0.30788 to 0.30541, saving model to best_model.hdf5\nEpoch 5/20\n140454/140454 [==============================] - 23s 161us/step - loss: 0.2844 - acc: 0.8696 - val_loss: 0.3051 - val_acc: 0.8581\n\nEpoch 00005: val_loss improved from 0.30541 to 0.30508, saving model to best_model.hdf5\nEpoch 6/20\n140454/140454 [==============================] - 23s 161us/step - loss: 0.2795 - acc: 0.8720 - val_loss: 0.3024 - val_acc: 0.8601\n\nEpoch 00006: val_loss improved from 0.30508 to 0.30238, saving model to best_model.hdf5\nEpoch 7/20\n140454/140454 [==============================] - 23s 160us/step - loss: 0.2745 - acc: 0.8745 - val_loss: 0.3020 - val_acc: 0.8602\n\nEpoch 00007: val_loss improved from 0.30238 to 0.30196, saving model to best_model.hdf5\nEpoch 8/20\n140454/140454 [==============================] - 23s 161us/step - loss: 0.2714 - acc: 0.8763 - val_loss: 0.3004 - val_acc: 0.8622\n\nEpoch 00008: val_loss improved from 0.30196 to 0.30036, saving model to best_model.hdf5\nEpoch 9/20\n140454/140454 [==============================] - 23s 161us/step - loss: 0.2683 - acc: 0.8778 - val_loss: 0.3018 - val_acc: 0.8602\n\nEpoch 00009: val_loss did not improve from 0.30036\nEpoch 10/20\n140454/140454 [==============================] - 23s 161us/step - loss: 0.2649 - acc: 0.8798 - val_loss: 0.3028 - val_acc: 0.8614\n\nEpoch 00010: val_loss did not improve from 0.30036\nEpoch 11/20\n140454/140454 [==============================] - 22s 160us/step - loss: 0.2634 - acc: 0.8802 - val_loss: 0.3023 - val_acc: 0.8615\n\nEpoch 00011: val_loss did not improve from 0.30036\n","output_type":"stream"}]},{"cell_type":"code","source":"model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)","metadata":{"_uuid":"b059392aad7d904adfb8ae151ad2004aa03da30d","execution":{"iopub.status.busy":"2024-03-20T01:36:31.201391Z","iopub.execute_input":"2024-03-20T01:36:31.201616Z","iopub.status.idle":"2024-03-20T01:42:57.352651Z","shell.execute_reply.started":"2024-03-20T01:36:31.201577Z","shell.execute_reply":"2024-03-20T01:42:57.351921Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Train on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 32s 229us/step - loss: 0.3542 - acc: 0.8407 - val_loss: 0.3219 - val_acc: 0.8527\n\nEpoch 00001: val_loss improved from inf to 0.32193, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 28s 200us/step - loss: 0.3195 - acc: 0.8549 - val_loss: 0.3108 - val_acc: 0.8549\n\nEpoch 00002: val_loss improved from 0.32193 to 0.31082, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 29s 206us/step - loss: 0.3095 - acc: 0.8588 - val_loss: 0.3103 - val_acc: 0.8548\n\nEpoch 00003: val_loss improved from 0.31082 to 0.31032, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 28s 202us/step - loss: 0.3019 - acc: 0.8624 - val_loss: 0.3012 - val_acc: 0.8593\n\nEpoch 00004: val_loss improved from 0.31032 to 0.30121, saving model to best_model.hdf5\nEpoch 5/20\n140454/140454 [==============================] - 28s 200us/step - loss: 0.2959 - acc: 0.8650 - val_loss: 0.3014 - val_acc: 0.8596\n\nEpoch 00005: val_loss did not improve from 0.30121\nEpoch 6/20\n140454/140454 [==============================] - 28s 200us/step - loss: 0.2905 - acc: 0.8672 - val_loss: 0.2992 - val_acc: 0.8594\n\nEpoch 00006: val_loss improved from 0.30121 to 0.29924, saving model to best_model.hdf5\nEpoch 7/20\n140454/140454 [==============================] - 28s 200us/step - loss: 0.2853 - acc: 0.8698 - val_loss: 0.3016 - val_acc: 0.8606\n\nEpoch 00007: val_loss did not improve from 0.29924\nEpoch 8/20\n140454/140454 [==============================] - 28s 200us/step - loss: 0.2817 - acc: 0.8716 - val_loss: 0.2969 - val_acc: 0.8628\n\nEpoch 00008: val_loss improved from 0.29924 to 0.29688, saving model to best_model.hdf5\nEpoch 9/20\n140454/140454 [==============================] - 28s 200us/step - loss: 0.2782 - acc: 0.8732 - val_loss: 0.3015 - val_acc: 0.8615\n\nEpoch 00009: val_loss did not improve from 0.29688\nEpoch 10/20\n140454/140454 [==============================] - 28s 200us/step - loss: 0.2758 - acc: 0.8747 - val_loss: 0.2968 - val_acc: 0.8626\n\nEpoch 00010: val_loss improved from 0.29688 to 0.29682, saving model to best_model.hdf5\nEpoch 11/20\n140454/140454 [==============================] - 28s 201us/step - loss: 0.2731 - acc: 0.8758 - val_loss: 0.3023 - val_acc: 0.8593\n\nEpoch 00011: val_loss did not improve from 0.29682\nEpoch 12/20\n140454/140454 [==============================] - 28s 201us/step - loss: 0.2709 - acc: 0.8771 - val_loss: 0.2992 - val_acc: 0.8625\n\nEpoch 00012: val_loss did not improve from 0.29682\nEpoch 13/20\n140454/140454 [==============================] - 28s 201us/step - loss: 0.2683 - acc: 0.8782 - val_loss: 0.3006 - val_acc: 0.8621\n\nEpoch 00013: val_loss did not improve from 0.29682\n","output_type":"stream"}]},{"cell_type":"code","source":"def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    \n    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n    \n    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n    \n    \n    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n    \n    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","metadata":{"_uuid":"8187e167ce93f0eb69f59cb9d7fedc4637a77cfe","execution":{"iopub.status.busy":"2024-03-20T01:42:57.354099Z","iopub.execute_input":"2024-03-20T01:42:57.354322Z","iopub.status.idle":"2024-03-20T01:42:57.368055Z","shell.execute_reply.started":"2024-03-20T01:42:57.354283Z","shell.execute_reply":"2024-03-20T01:42:57.367057Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)","metadata":{"_uuid":"9bf90d6d4effebb3c5aa9b666b8e09c9c57d94d3","execution":{"iopub.status.busy":"2024-03-20T01:42:57.369221Z","iopub.execute_input":"2024-03-20T01:42:57.369471Z","iopub.status.idle":"2024-03-20T01:50:27.954736Z","shell.execute_reply.started":"2024-03-20T01:42:57.369423Z","shell.execute_reply":"2024-03-20T01:50:27.954096Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Train on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 30s 213us/step - loss: 0.6286 - acc: 0.6429 - val_loss: 0.4549 - val_acc: 0.8219\n\nEpoch 00001: val_loss improved from inf to 0.45489, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 25s 176us/step - loss: 0.4260 - acc: 0.8100 - val_loss: 0.3708 - val_acc: 0.8402\n\nEpoch 00002: val_loss improved from 0.45489 to 0.37077, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 25s 177us/step - loss: 0.3751 - acc: 0.8337 - val_loss: 0.3416 - val_acc: 0.8451\n\nEpoch 00003: val_loss improved from 0.37077 to 0.34164, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 25s 179us/step - loss: 0.3522 - acc: 0.8419 - val_loss: 0.3269 - val_acc: 0.8491\n\nEpoch 00004: val_loss improved from 0.34164 to 0.32693, saving model to best_model.hdf5\nEpoch 5/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3414 - acc: 0.8459 - val_loss: 0.3211 - val_acc: 0.8514\n\nEpoch 00005: val_loss improved from 0.32693 to 0.32114, saving model to best_model.hdf5\nEpoch 6/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3346 - acc: 0.8490 - val_loss: 0.3203 - val_acc: 0.8505\n\nEpoch 00006: val_loss improved from 0.32114 to 0.32027, saving model to best_model.hdf5\nEpoch 7/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3288 - acc: 0.8510 - val_loss: 0.3194 - val_acc: 0.8510\n\nEpoch 00007: val_loss improved from 0.32027 to 0.31943, saving model to best_model.hdf5\nEpoch 8/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3246 - acc: 0.8528 - val_loss: 0.3169 - val_acc: 0.8539\n\nEpoch 00008: val_loss improved from 0.31943 to 0.31690, saving model to best_model.hdf5\nEpoch 9/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3218 - acc: 0.8538 - val_loss: 0.3129 - val_acc: 0.8543\n\nEpoch 00009: val_loss improved from 0.31690 to 0.31289, saving model to best_model.hdf5\nEpoch 10/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3185 - acc: 0.8552 - val_loss: 0.3126 - val_acc: 0.8543\n\nEpoch 00010: val_loss improved from 0.31289 to 0.31257, saving model to best_model.hdf5\nEpoch 11/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3155 - acc: 0.8563 - val_loss: 0.3121 - val_acc: 0.8552\n\nEpoch 00011: val_loss improved from 0.31257 to 0.31212, saving model to best_model.hdf5\nEpoch 12/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3136 - acc: 0.8575 - val_loss: 0.3151 - val_acc: 0.8550\n\nEpoch 00012: val_loss did not improve from 0.31212\nEpoch 13/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3112 - acc: 0.8583 - val_loss: 0.3093 - val_acc: 0.8568\n\nEpoch 00013: val_loss improved from 0.31212 to 0.30933, saving model to best_model.hdf5\nEpoch 14/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3095 - acc: 0.8592 - val_loss: 0.3074 - val_acc: 0.8579\n\nEpoch 00014: val_loss improved from 0.30933 to 0.30741, saving model to best_model.hdf5\nEpoch 15/20\n140454/140454 [==============================] - 25s 177us/step - loss: 0.3074 - acc: 0.8602 - val_loss: 0.3082 - val_acc: 0.8572\n\nEpoch 00015: val_loss did not improve from 0.30741\nEpoch 16/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3056 - acc: 0.8604 - val_loss: 0.3128 - val_acc: 0.8559\n\nEpoch 00016: val_loss did not improve from 0.30741\nEpoch 17/20\n140454/140454 [==============================] - 25s 178us/step - loss: 0.3035 - acc: 0.8616 - val_loss: 0.3224 - val_acc: 0.8542\n\nEpoch 00017: val_loss did not improve from 0.30741\n","output_type":"stream"}]},{"cell_type":"code","source":"model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)","metadata":{"_uuid":"bf6d8d367c5adc30e00bbd77c1de70fd52960441","execution":{"iopub.status.busy":"2024-03-20T01:50:27.957810Z","iopub.execute_input":"2024-03-20T01:50:27.958074Z","iopub.status.idle":"2024-03-20T01:55:53.921569Z","shell.execute_reply.started":"2024-03-20T01:50:27.958030Z","shell.execute_reply":"2024-03-20T01:55:53.920817Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Train on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 30s 215us/step - loss: 0.3603 - acc: 0.8401 - val_loss: 0.3202 - val_acc: 0.8514\n\nEpoch 00001: val_loss improved from inf to 0.32019, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 24s 174us/step - loss: 0.3245 - acc: 0.8530 - val_loss: 0.3108 - val_acc: 0.8543\n\nEpoch 00002: val_loss improved from 0.32019 to 0.31076, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 24s 174us/step - loss: 0.3135 - acc: 0.8573 - val_loss: 0.3080 - val_acc: 0.8565\n\nEpoch 00003: val_loss improved from 0.31076 to 0.30800, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 25s 175us/step - loss: 0.3061 - acc: 0.8598 - val_loss: 0.3057 - val_acc: 0.8581\n\nEpoch 00004: val_loss improved from 0.30800 to 0.30566, saving model to best_model.hdf5\nEpoch 5/20\n140454/140454 [==============================] - 25s 175us/step - loss: 0.3003 - acc: 0.8625 - val_loss: 0.3071 - val_acc: 0.8578\n\nEpoch 00005: val_loss did not improve from 0.30566\nEpoch 6/20\n140454/140454 [==============================] - 25s 175us/step - loss: 0.2956 - acc: 0.8651 - val_loss: 0.3038 - val_acc: 0.8605\n\nEpoch 00006: val_loss improved from 0.30566 to 0.30378, saving model to best_model.hdf5\nEpoch 7/20\n140454/140454 [==============================] - 25s 175us/step - loss: 0.2914 - acc: 0.8668 - val_loss: 0.3027 - val_acc: 0.8589\n\nEpoch 00007: val_loss improved from 0.30378 to 0.30271, saving model to best_model.hdf5\nEpoch 8/20\n140454/140454 [==============================] - 25s 175us/step - loss: 0.2874 - acc: 0.8686 - val_loss: 0.2984 - val_acc: 0.8615\n\nEpoch 00008: val_loss improved from 0.30271 to 0.29835, saving model to best_model.hdf5\nEpoch 9/20\n140454/140454 [==============================] - 25s 175us/step - loss: 0.2843 - acc: 0.8701 - val_loss: 0.2977 - val_acc: 0.8615\n\nEpoch 00009: val_loss improved from 0.29835 to 0.29774, saving model to best_model.hdf5\nEpoch 10/20\n140454/140454 [==============================] - 25s 175us/step - loss: 0.2817 - acc: 0.8718 - val_loss: 0.2997 - val_acc: 0.8613\n\nEpoch 00010: val_loss did not improve from 0.29774\nEpoch 11/20\n140454/140454 [==============================] - 25s 175us/step - loss: 0.2795 - acc: 0.8729 - val_loss: 0.3003 - val_acc: 0.8613\n\nEpoch 00011: val_loss did not improve from 0.29774\nEpoch 12/20\n140454/140454 [==============================] - 25s 175us/step - loss: 0.2770 - acc: 0.8740 - val_loss: 0.3030 - val_acc: 0.8599\n\nEpoch 00012: val_loss did not improve from 0.29774\n","output_type":"stream"}]},{"cell_type":"code","source":"model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)","metadata":{"_uuid":"b10113439be683bf19930750eaf96328e5b58d42","execution":{"iopub.status.busy":"2024-03-20T01:55:53.923063Z","iopub.execute_input":"2024-03-20T01:55:53.923380Z","iopub.status.idle":"2024-03-20T02:00:25.651940Z","shell.execute_reply.started":"2024-03-20T01:55:53.923322Z","shell.execute_reply":"2024-03-20T02:00:25.651182Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Train on 140454 samples, validate on 15606 samples\nEpoch 1/20\n140454/140454 [==============================] - 33s 234us/step - loss: 0.3802 - acc: 0.8280 - val_loss: 0.3176 - val_acc: 0.8522\n\nEpoch 00001: val_loss improved from inf to 0.31759, saving model to best_model.hdf5\nEpoch 2/20\n140454/140454 [==============================] - 26s 186us/step - loss: 0.3198 - acc: 0.8555 - val_loss: 0.3145 - val_acc: 0.8544\n\nEpoch 00002: val_loss improved from 0.31759 to 0.31449, saving model to best_model.hdf5\nEpoch 3/20\n140454/140454 [==============================] - 26s 185us/step - loss: 0.3066 - acc: 0.8601 - val_loss: 0.3066 - val_acc: 0.8572\n\nEpoch 00003: val_loss improved from 0.31449 to 0.30660, saving model to best_model.hdf5\nEpoch 4/20\n140454/140454 [==============================] - 26s 184us/step - loss: 0.2975 - acc: 0.8638 - val_loss: 0.3062 - val_acc: 0.8586\n\nEpoch 00004: val_loss improved from 0.30660 to 0.30625, saving model to best_model.hdf5\nEpoch 5/20\n140454/140454 [==============================] - 26s 186us/step - loss: 0.2890 - acc: 0.8677 - val_loss: 0.3035 - val_acc: 0.8597\n\nEpoch 00005: val_loss improved from 0.30625 to 0.30355, saving model to best_model.hdf5\nEpoch 6/20\n140454/140454 [==============================] - 26s 185us/step - loss: 0.2831 - acc: 0.8704 - val_loss: 0.3028 - val_acc: 0.8625\n\nEpoch 00006: val_loss improved from 0.30355 to 0.30285, saving model to best_model.hdf5\nEpoch 7/20\n140454/140454 [==============================] - 26s 185us/step - loss: 0.2781 - acc: 0.8725 - val_loss: 0.3052 - val_acc: 0.8608\n\nEpoch 00007: val_loss did not improve from 0.30285\nEpoch 8/20\n140454/140454 [==============================] - 26s 185us/step - loss: 0.2732 - acc: 0.8749 - val_loss: 0.3062 - val_acc: 0.8600\n\nEpoch 00008: val_loss did not improve from 0.30285\nEpoch 9/20\n140454/140454 [==============================] - 26s 184us/step - loss: 0.2697 - acc: 0.8770 - val_loss: 0.3066 - val_acc: 0.8606\n\nEpoch 00009: val_loss did not improve from 0.30285\n","output_type":"stream"}]},{"cell_type":"code","source":"pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\npred = pred1\npred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred2\npred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred3\npred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred4\npred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\npred += pred5","metadata":{"_uuid":"8529014d1a239f308ac5f2552088ce2d8ebb8966","execution":{"iopub.status.busy":"2024-03-20T02:00:25.653341Z","iopub.execute_input":"2024-03-20T02:00:25.653565Z","iopub.status.idle":"2024-03-20T02:00:42.483108Z","shell.execute_reply.started":"2024-03-20T02:00:25.653527Z","shell.execute_reply":"2024-03-20T02:00:42.482426Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"66292/66292 [==============================] - 3s 51us/step\n66292/66292 [==============================] - 4s 56us/step\n66292/66292 [==============================] - 3s 49us/step\n66292/66292 [==============================] - 3s 48us/step\n66292/66292 [==============================] - 3s 49us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = np.round(np.argmax(pred, axis=1)).astype(int)\nsub['Sentiment'] = predictions\nsub.to_csv(\"blend.csv\", index=False)","metadata":{"_uuid":"d58a5b52ea647dab51123ef89878c5355b3d2971","execution":{"iopub.status.busy":"2024-03-20T02:00:42.484656Z","iopub.execute_input":"2024-03-20T02:00:42.484982Z","iopub.status.idle":"2024-03-20T02:00:42.965471Z","shell.execute_reply.started":"2024-03-20T02:00:42.484925Z","shell.execute_reply":"2024-03-20T02:00:42.964576Z"},"trusted":true},"execution_count":42,"outputs":[]}]}